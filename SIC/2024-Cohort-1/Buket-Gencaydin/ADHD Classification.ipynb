{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install onnx\n",
    "!pip3 install tensorflow\n",
    "!pip3 install nilearn\n",
    "!pip3 install scikit-learn\n",
    "!pip3 install --upgrade scikit-learn imbalanced-learn\n",
    "!pip3 install pytest\n",
    "!pip3 install nipype\n",
    "!pip3 install --upgrade tensorflow keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install --upgrade tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/buketgencaydin/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "\n",
    "import onnx\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "from nilearn.input_data import NiftiMasker\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, LeaveOneGroupOut\n",
    "from keras.layers import TimeDistributed, LSTM\n",
    "from nilearn.masking import apply_mask, unmask, compute_epi_mask\n",
    "from nilearn.image import resample_to_img, index_img\n",
    "from nibabel.testing import data_path\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import argparse\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import keras.layers\n",
    "import keras.models\n",
    "import keras.utils\n",
    "from keras.optimizers import Adam\n",
    "import nipype.interfaces.io as nio\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import scipy.io as sio\n",
    "from nilearn import image, plotting\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# For TensorFlow 1.x compatibility\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "# Use to_categorical from tensorflow.keras.utils instead of np_utils\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KKI DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting labels\n",
    "phenotypic = pd.read_csv('/Users/buketgencaydin/Downloads/data/KKI_phenotypic.csv')\n",
    "labels = phenotypic['DX']\n",
    "labels = pd.DataFrame(labels)\n",
    "ID = phenotypic['ScanDir ID']\n",
    "ID = pd.DataFrame(ID)\n",
    "result = pd.concat([ID, labels], axis=1, join='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting fMRI image path of one KKI site\n",
    "nifti_files = glob.glob('/Users/buketgencaydin/Downloads/data/fmri_KKI/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating IDs from the list of paths\n",
    "s_nifti = []\n",
    "for i in nifti_files:\n",
    "    s_nifti.append(re.split('(\\d+)', i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separting ID from the list of path \n",
    "s1_nifti = []\n",
    "for i in range(len(s_nifti)):\n",
    "    s1_nifti.append(s_nifti[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voxel dimensions:\n",
      "  x = 3.0 mm\n",
      "  y = 3.0 mm\n",
      "  z = 3.0 mm\n",
      "Number of non-zero voxels = 18927572\n",
      "Volume of non-zero voxels = 511044444.0 mm^3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nibabel as nb\n",
    "\n",
    "# Load data\n",
    "nii = nb.load(nifti_files[0])\n",
    "img = nii.get_fdata()\n",
    "\n",
    "# Get voxel dimensions\n",
    "voxel_dims = (nii.header[\"pixdim\"])[1:4]\n",
    "print(\"Voxel dimensions:\")\n",
    "print(\"  x = {} mm\".format(voxel_dims[0]))\n",
    "print(\"  y = {} mm\".format(voxel_dims[1]))\n",
    "print(\"  z = {} mm\".format(voxel_dims[2]))\n",
    "\n",
    "# Compute volume\n",
    "nonzero_voxel_count = np.count_nonzero(img)\n",
    "voxel_volume = np.prod(voxel_dims)\n",
    "nonzero_voxel_volume = nonzero_voxel_count * voxel_volume\n",
    "\n",
    "print(\"Number of non-zero voxels = {}\".format(nonzero_voxel_count))\n",
    "print(\"Volume of non-zero voxels = {} mm^3\".format(nonzero_voxel_volume))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nilearn.maskers.nifti_masker.NiftiMasker"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "from nilearn.image import index_img\n",
    "\n",
    "\n",
    "epi_img = index_img(nifti_files[0], slice(0, 100))\n",
    "masker = NiftiMasker(mask_strategy='epi')\n",
    "s1 = masker.fit(epi_img)\n",
    "type(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fmri_masked' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfmri_masked\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fmri_masked' is not defined"
     ]
    }
   ],
   "source": [
    "fmri_masked.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from nilearn import image\n",
    "\n",
    "# Load the first NIfTI file\n",
    "Img_1 = nib.load(nifti_files[0])\n",
    "\n",
    "# Index the image\n",
    "Niak_1 = image.index_img(Img_1, 1)\n",
    "\n",
    "# Print the shape of the indexed image\n",
    "print(Niak_1.shape)\n",
    "\n",
    "# Retrieve the header information\n",
    "Niak_info = Niak_1.header\n",
    "print(Niak_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sorting labels according to file\n",
    "\n",
    "Id = result['ScanDir ID'].values.tolist()\n",
    "label = result['DX'].values.tolist()\n",
    "#df2 = s1_nifti[0].values.tolist() \n",
    "\n",
    "df_sort = []\n",
    "for i in range(len(s1_nifti)):\n",
    "    df_sort.append(int(s1_nifti[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting the labels according to file\n",
    "\n",
    "new_list = []\n",
    "for i in range(len(df_sort)):\n",
    "    if df_sort[i] in Id:\n",
    "        s = Id.index(df_sort[i])\n",
    "        #print(df_sort[i])\n",
    "        new_list.append(label[s])\n",
    "        #print(label[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 64, 46)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Img = nib.load('/Users/buketgencaydin/Downloads/data/KKI_data/' + s1_nifti[2] +'.nii.gz')\n",
    "x = Img.get_fdata()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing Visualization of one instance without ADHD\n",
    "data = arr \n",
    "mean_func = image.mean_img(Niak_1)\n",
    "\n",
    "#smoothing  with a varying amount of smoothing, from none to 14mm by increment of 2mm\n",
    "for smoothing in range(0, 3, 1):\n",
    "    smoothed_img = image.smooth_img(mean_func, smoothing)\n",
    "    plotting.plot_epi(smoothed_img,title=\"Smoothing %imm\" % smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing smoothing and cleaning of image on whole dataset and saving it to another folder\n",
    "for i in range(len(nifti_files)):\n",
    "    smoothed_img = image.smooth_img(nifti_files[i], 2)\n",
    "    s = image.clean_img(smoothed_img, runs=None, detrend=True ,standardize= True)\n",
    "    s2 = image.index_img(s,10)\n",
    "    nib.save(s2 , '/Users/buketgencaydin/Downloads/data/KKI_data/' + s1_nifti[i] +'.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing 2mm smoothing on whole data\n",
    "#reshaping the dimensions of the data before passing to  cnn\n",
    "Dataset = []\n",
    "for i in range(83):\n",
    "    Img = nib.load('/Users/buketgencaydin/Downloads/data/KKI_data/' + s1_nifti[i] +'.nii.gz')\n",
    "    x = Img.get_fdata()\n",
    "    #result = x[:, :, :,:1]\n",
    "    Dataset.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the dimensions of the data before passing to  cnn\n",
    "Dataset = []\n",
    "for i in range(83):\n",
    "    Img = nib.load('/Users/buketgencaydin/Downloads/data/KKI_data/' + s1_nifti[i] +'.nii.gz')\n",
    "    x = Img.get_fdata()\n",
    "    #result = x[:, :, :,:1]\n",
    "    Dataset.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(Dataset)\n",
    "Y = np.array(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary classification\n",
    "for i in range(len(new_list)):\n",
    "    if new_list[i] > 1:\n",
    "        new_list[i] = 1\n",
    "    else:\n",
    "        continue \n",
    "        \n",
    "#X = np.array([Dataset], dtype = object)\n",
    "#Y = np.array(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 61, 1: 22})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83, 53, 64, 46)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combining both dataset\n",
    "#x_combined = np.concatenate((X, X_ohsu))\n",
    "#y_combined = np.concatenate((Y,Y_ohsu))\n",
    "\n",
    "x_combined = X\n",
    "y_combined = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 61, 1: 22})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using counter to check if data is balanced or not\n",
    "Counter(y_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping the train data value\n",
    "reshaped_data = []\n",
    "for i in range(len(x_combined)):\n",
    "    reshaped_data.append(x_combined[i].reshape(1,53,64,46))\n",
    "X_data = np.array(reshaped_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to categorical\n",
    "Y_new = keras.utils.to_categorical(y_combined, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_data,Y_new,shuffle = True, random_state = 32, test_size=0.15)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import TimeDistributed, Conv2D, Activation, MaxPool2D, Dropout, Flatten, LSTM, Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=3, kernel_initializer='uniform', padding='same'),\n",
    "                          input_shape=(None, 53, 64, 46)))  # Ensure this input shape matches your data\n",
    "\n",
    "model.add(TimeDistributed(Activation(\"relu\")))\n",
    "model.add(TimeDistributed(MaxPool2D(pool_size=2)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=3, kernel_initializer='uniform', padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPool2D(pool_size=2)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=3, kernel_initializer='uniform', padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPool2D(pool_size=2)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=3, kernel_initializer='uniform', padding='same', activation='relu')))\n",
    "model.add(TimeDistributed(MaxPool2D(pool_size=2)))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=3, kernel_initializer='uniform', padding='same', activation='relu')))\n",
    "\n",
    "model.add(TimeDistributed(Conv2D(32, kernel_size=3, kernel_initializer='uniform', padding='same', activation='relu')))\n",
    "\n",
    "model.add(TimeDistributed(Dropout(0.2)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "\n",
    "model.add(Dense(250, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dense(250, kernel_initializer='he_normal', activation='relu'))\n",
    "model.add(Dense(4))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Initialize the Adam optimizer\n",
    "adam = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.1)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "#print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "`tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.9/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:503\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m iterator_ops\u001b[38;5;241m.\u001b[39mOwnedIterator(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: `tf.data.Dataset` only supports Python-style iteration in eager mode or within tf.function."
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train,epochs= 200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation\n",
    "Kfold = KFold(n_splits=7)\n",
    "Kfold.get_n_splits(x_train, y_train)  \n",
    "foldNum=0                                  #initializing fold = 0\n",
    "for train_index, val_index in Kfold.split(x_train, y_train):\n",
    "    foldNum+=1\n",
    "    print(\"fold\",foldNum)\n",
    "    X_train, X_val = x_train[train_index], x_train[val_index]\n",
    "    Y_train, Y_val = y_train[train_index], y_train[val_index]\n",
    "    print(X_train.shape)\n",
    "    #training the dataset\n",
    "    history1 = model.fit(X_train, Y_train, validation_data = (X_val,Y_val), epochs=20,batch_size=32, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('FMRI_model.h5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('FMRI_weights.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#previous model \n",
    "model = keras.models.Sequential() \n",
    "model.add(keras.layers.Conv2D(32, kernel_size= 3, \n",
    "                                              kernel_initializer='he_normal',padding='same',\n",
    "                                              input_shape=(53,64,46)))\n",
    "model.add(keras.layers.Activation(\"relu\"))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(keras.layers.Conv2D(32, kernel_size= 3, kernel_initializer='he_normal', padding = 'same',\n",
    "                              activation = 'relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size= 3, kernel_initializer='he_normal',padding = 'same',\n",
    "                              activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size = 2))\n",
    "\n",
    "model.add(keras.layers.Conv2D(64, kernel_size= 3, kernel_initializer='he_normal', padding = 'same',\n",
    "                              activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(pool_size=2))\n",
    "\n",
    "\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "model.add(keras.layers.Dense(250, kernel_initializer='he_normal',activation = 'relu'))\n",
    "model.add(keras.layers.Dense(250, kernel_initializer='he_normal',activation = 'relu'))\n",
    "model.add(keras.layers.Dense(250, kernel_initializer='he_normal',activation = 'relu'))\n",
    "model.add(keras.layers.Dense(2,activation = 'softmax'))\n",
    "#model.add(Activation('linear'))\n",
    "    \n",
    "#model.add(keras.layers.Activation('softmax'))\n",
    "    \n",
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.1)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
